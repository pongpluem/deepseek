# การนำโมเดล DeepSeek ที่ Fine-tune แล้วมาใช้งาน

หลังจาก fine-tune โมเดล DeepSeek เสร็จแล้ว คุณสามารถนำมาใช้งานได้ดังนี้:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# 1. โหลดโมเดลที่ fine-tune แล้ว
model_path = "./my-finetuned-deepseek"  # ตำแหน่งที่เก็บโมเดลที่ fine-tune แล้ว
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,  # ใช้ความแม่นยำครึ่งเพื่อประหยัดหน่วยความจำ
    device_map="auto",          # จัดสรรโมเดลไปยังอุปกรณ์ที่มีอยู่โดยอัตโนมัติ
)

# 2. สร้างฟังก์ชันสำหรับการสร้างข้อความ
def generate_text(prompt, max_new_tokens=512, temperature=0.7):
    # รูปแบบ prompt สำหรับ DeepSeek
    formatted_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    # สร้าง input tokens
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)
    
    # ทำการสร้างข้อความ
    with torch.no_grad():
        outputs = model.generate(
            inputs["input_ids"],
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=0.9,
            repetition_penalty=1.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    # แปลง tokens กลับเป็นข้อความ
    full_output = tokenizer.decode(outputs[0], skip_special_tokens=False)
    
    # ดึงเฉพาะส่วนคำตอบของ assistant ออกมา
    assistant_response = full_output.split("<|im_start|>assistant\n")[-1].replace("<|im_end|>", "")
    
    return assistant_response.strip()

# 3. ทดลองใช้งาน
response = generate_text("สวัสดี ช่วยอธิบายเรื่องการเรียนรู้ของเครื่องให้หน่อย")
print(response)
```

## การปรับใช้งานในสภาพแวดล้อมต่างๆ

### 1. ใช้งานเป็น API ด้วย FastAPI

```python
from fastapi import FastAPI, Request
from pydantic import BaseModel
import uvicorn

app = FastAPI()

class QueryRequest(BaseModel):
    prompt: str
    max_tokens: int = 512
    temperature: float = 0.7

@app.post("/generate")
async def generate(request: QueryRequest):
    response = generate_text(
        request.prompt, 
        max_new_tokens=request.max_tokens,
        temperature=request.temperature
    )
    return {"response": response}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 2. ใช้งานใน Streamlit เพื่อสร้าง UI แบบง่าย

```python
import streamlit as st

st.title("DeepSeek AI Chat")

# เริ่มต้นการสนทนา
if "messages" not in st.session_state:
    st.session_state.messages = []

# แสดงประวัติการสนทนา
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# รับข้อความจากผู้ใช้
if prompt := st.chat_input("พิมพ์ข้อความ..."):
    # เพิ่มข้อความของผู้ใช้ลงในประวัติ
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # แสดงข้อความของผู้ใช้
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # สร้างคำตอบ
    with st.chat_message("assistant"):
        response = generate_text(prompt)
        st.markdown(response)
    
    # เพิ่มคำตอบลงในประวัติ
    st.session_state.messages.append({"role": "assistant", "content": response})
```

### 3. การปรับแต่งประสิทธิภาพ

1. **การบีบอัดโมเดล**:
   ```python
   # ใช้ 8-bit quantization ให้โมเดลเล็กลง
   model = AutoModelForCausalLM.from_pretrained(
       model_path,
       load_in_8bit=True,
       device_map="auto",
   )
   ```

2. **การทำงานบน CPU** (เมื่อไม่มี GPU):
   ```python
   model = AutoModelForCausalLM.from_pretrained(
       model_path,
       device_map="cpu",
       torch_dtype=torch.float32,  # ใช้ float32 สำหรับ CPU
   )
   ```

3. **การใช้งานใน Production**:
   - พิจารณาใช้ ONNX Runtime หรือ TensorRT เพื่อเร่งความเร็ว
   - จัดการการจอง resource โดยใช้ queuing system

คุณสามารถปรับแต่งพารามิเตอร์ต่างๆ เช่น temperature, top_p, และ repetition_penalty เพื่อให้ได้ผลลัพธ์ที่เหมาะสมกับงานของคุณ